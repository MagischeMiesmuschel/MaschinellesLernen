{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nearest-Neighbor Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame([\n",
    "    [  1 ,   7 ,  1 ,   1 ,  \"spo\" ],\n",
    "    [  2 ,  13 ,  0 ,   1 ,  \"spo\" ],\n",
    "    [  3 ,   8 ,  2 ,   1 ,  \"spo\" ],\n",
    "    [  4 ,   7 ,  2 ,   3 ,  \"spo\" ],\n",
    "    [  5 ,   2 ,  1 ,   0 ,  \"sci\" ],\n",
    "    [  6 ,   1 ,  0 ,   1 ,  \"sci\" ],\n",
    "    [  7 ,   2 ,  2 ,   1 ,  \"sci\" ],\n",
    "    [  8 ,  12 ,  8 ,  11 ,  \"his\" ],\n",
    "    [  9 ,   7 ,  3 ,   5 ,  \"his\" ],\n",
    "    [ 10 ,   6 ,  5 ,   8 ,  \"his\" ]],\n",
    "    columns=[  \"DocId\" ,  \"$X_p$\" ,  \"$X_l$\" ,  \"$X_d$\" ,  \"$Y$\" ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are given a collection of text documents that each have been classified according to their content into the three classes\n",
    "*sports*, *scientific*, and *historical*.\n",
    "\n",
    "Furthermore, three integer features $(X_p, X_l, X_d)$ have been obtained for each document,\n",
    "by detecting named entities and tagging each entity as *person*, *location*, or *date*,\n",
    "and counting the number of times each kind of entity occurs.\n",
    "\n",
    "For each training document, you know $X_p, X_l, X_d$ and the class label $Y = \\{\\text{spo}, \\text{sci}, \\text{his}\\}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Compute the most likely class for $x=(5,4,3)$ using the $k$-nearest neighbor classifier with $k=3$ and show how you arrived at your solution.\n",
    "Use the Euclidean distance function and majority voting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Reduce the data set to the mean vectors of each class only.\n",
    "Compute the most likely class for $x$ using the nearest-neighbor classifier on this reduced data set\n",
    "and show how you arrived at your solution. Use the Euclidean distance function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Briefly discuss why the results of the $k$-nearest neighbor classifier could be different from\n",
    "the results obtained by using the nearest-mean classifier from the previous part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probabilistic Tic-Tac-Toe\n",
    "\n",
    "In this exercise we will train a probabilistic model to play Tic-Tac-Toe with us. To do so, download the UCI Tic-Tac-Toe dataset (https://archive.ics.uci.edu/ml/datasets/Tic-Tac-Toe+Endgame)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Load the data and make yourself familiar with the data representation. Write code to display a field in your shell using ASCII characters. Write a function `chose_cell` that given the data and using Bayes' Theorem, returns the best cell (as number from 0 to 8) for the first turn of player $X$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('tic-tac-toe.data')\n",
    "\n",
    "def display_field(field):\n",
    "    # TODO\n",
    "\n",
    "def chose_cell(df):\n",
    "    # TODO\n",
    "\n",
    "print(chose_cell(df))\n",
    "test_field = ['b' for i in range(9)]\n",
    "test_field[chose_cell(df)] = 'x'\n",
    "display_field(test_field)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Adapt your function `chose_cell` to also take the current board situation into consideration and returns the best *blank* cell (as number from 0 to 8) for any turn of player $X$. Use the interactive shell to play against it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chose_cell(field,df):\n",
    "    # TODO\n",
    "\n",
    "# Game functions\n",
    "game_field = []\n",
    "def init_game_field():\n",
    "    global game_field # Uses global variables instead of local\n",
    "    game_field = ['b' for i in range(9)]\n",
    "    make_ai_turn()\n",
    "\n",
    "def make_ai_turn():\n",
    "    global df, game_field\n",
    "    # TODO\n",
    "    display_field(game_field)\n",
    "\n",
    "def make_human_turn(position):\n",
    "    global game_field\n",
    "    # TODO\n",
    "    display_field(game_field)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Play here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Create a function `chose_cell_lookahead` that returns the best *blank* cell, for which winning probabilities of player $X$ are best after the player $O$'s turn using Bayes' Theorem. Do you expect to obtain different results? Test your theories by playing against both functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chose_cell_lookahead(field,df):\n",
    "    # TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Play here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Traffic Count imputation\n",
    "\n",
    "In this exercise we want to use the K Nearest-Neighbor algorithm for traffic prediction. In moodle you can find the file `luxembourg_counts.csv` which contains simulated data from the city of Luxembourg. Our task is to predict the average number of vehicles (counts) in a given region from the morning hours 7:00 to 11:00. To tackle this problem, Luxembourg has been split into multiple grid cells. Each row in `luxembourg_counts.csv` represents one grid cell (identified by its latitude and logitude coordinates) followed by its average vehicle count in the given time slice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Load the data and plot a histogram of traffic counts. Compute a $20\\%$ test split, which will be used to judge the algorithms afterwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('luxembourg_counts.csv')\n",
    "\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Implement the Mean-Squared-Error function:\n",
    "$$\n",
    "MSE = \\frac{1}{N_{Test}} \\sum_{(x,y)\\in D_{Test}} (f(x) - y)^2\n",
    "$$\n",
    "and a simple baseline model, which always predicts the mean\n",
    "$$\n",
    "f_{base}(x) = \\frac{1}{N_{Train}} \\sum_{(x,y)\\in D_{Train}} y\n",
    "$$\n",
    "Test its performance with a $10-$fold cross validation and the test set. \n",
    "\n",
    "*Note: The jupyter notebook file already contains the necessary template to integrate your own classifier into `sklearn`*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "from sklearn.utils.validation import check_X_y, check_array, check_is_fitted\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "\n",
    "class BaselineModel(BaseEstimator, RegressorMixin):\n",
    "    def __init__(self):\n",
    "        #We do not have any parameters\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        # Check that X and y have correct shape\n",
    "        X, y = check_X_y(X, y)\n",
    "\n",
    "        # Store the classes seen during fit. Since we will do regression, this is not necessary\n",
    "        # self.classes_ = unique_labels(y)\n",
    "\n",
    "        self.X_ = X\n",
    "        self.y_ = y\n",
    "\n",
    "        # The actual classifier's fit method\n",
    "        # TODO\n",
    "\n",
    "        # Return the classifier\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Check is fit had been called\n",
    "        check_is_fitted(self, ['X_', 'y_'])\n",
    "\n",
    "        # Input validation\n",
    "        X = check_array(X)\n",
    "        \n",
    "        # actual predict method\n",
    "        # TODO\n",
    "        \n",
    "        return 0 # return prediction here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "def mse(Y,YPredicted):\n",
    "    # TODO\n",
    "\n",
    "\n",
    "my_scorer = make_scorer(mse)\n",
    "\n",
    "xvalScores = []\n",
    "\n",
    "meanModel = BaselineModel();\n",
    "scores = cross_val_score(meanModel, XTrain, YTrain, cv = 10, scoring=my_scorer) \n",
    "score = np.mean(scores)\n",
    "\n",
    "meanModel = BaselineModel();\n",
    "meanModel.fit(XTrain,YTrain)\n",
    "print(\"MSE training = \", score)    \n",
    "print(\"MSE test = \", mse(YTest, meanModel.predict(XTest)))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Traffic counts have a large variance, which makes the results difficult to interpret. The Standardized Mean Squared Error\n",
    "$$\n",
    "SMSE = \\frac{1}{N_{Test}\\cdot var( D_{Test})} \\sum_{(x,y)\\in D_{Test}} (f(x) - y)^2\n",
    "$$\n",
    "standardized the MSE by also taking into account the variance $var( D_{Test})$ of the lables in the test data. Implement the SMSE and use it to rate the baseline model. What happens and why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "def smse(Y,YPred):\n",
    "    # TODO\n",
    "\n",
    "my_scorer = make_scorer(smse)\n",
    "\n",
    "xvalScores = []\n",
    "\n",
    "meanModel = BaselineModel();\n",
    "scores = cross_val_score(meanModel, XTrain, YTrain, cv = 10, scoring=my_scorer) \n",
    "score = np.mean(scores)\n",
    "\n",
    "meanModel = BaselineModel();\n",
    "meanModel.fit(XTrain,YTrain)\n",
    "print(\"MSE training = \", score)    \n",
    "print(\"MSE test = \", mse(YTest, meanModel.predict(XTest)))     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d) Implement the K-Nearest-Neigbour algorithm and try different $K\\in\\{1,\\dots,20\\}$. What is the best configuration and what is its performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "from sklearn.utils.validation import check_X_y, check_array, check_is_fitted\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "\n",
    "class MyKNN(BaseEstimator, RegressorMixin):\n",
    "    def __init__(self, K):\n",
    "        self.K = K\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Check that X and y have correct shape\n",
    "        X, y = check_X_y(X, y)\n",
    "\n",
    "        # Store the classes seen during fit. Since we will do regression, this is not necessary\n",
    "        #self.classes_ = unique_labels(y)\n",
    "\n",
    "        self.X_ = X\n",
    "        self.y_ = y\n",
    "\n",
    "        # The actual classifier's fit method\n",
    "        # TODO\n",
    "\n",
    "        # Return the classifier\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Check is fit had been called\n",
    "        check_is_fitted(self, ['X_', 'y_'])\n",
    "\n",
    "        # Input validation\n",
    "        X = check_array(X)\n",
    "\n",
    "        # actual predict method\n",
    "        # TODO\n",
    "        return 0 # return prediction here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Validating on parameters and chosing best."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
